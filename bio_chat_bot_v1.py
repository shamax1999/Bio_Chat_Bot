# -*- coding: utf-8 -*-
"""BIO_CHAT_BOT_V1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18zyTBRqXRiWijGYa2P5J1R7SQww9pIH7
"""

!pip install gradio langchain langchain_groq langchain_community faiss-cpu Pillow pytesseract pdf2image PyPDF2 sentence_transformers groq tiktoken
!apt-get update
!apt-get install -y poppler-utils tesseract-ocr

"""Mount Google Drive"""

from google.colab import drive
import os

drive.mount('/content/drive')

drive_base = "/content/drive/MyDrive/BioChatbot"
pdf_folder = os.path.join(drive_base, "pdfs")
img_folder = os.path.join(drive_base, "pdf_images")
os.makedirs(pdf_folder, exist_ok=True)
os.makedirs(img_folder, exist_ok=True)

print(f"Data will be saved in Google Drive under: {drive_base}")

"""Upload PDFs into Drive"""

from google.colab import files
uploaded = files.upload()

for filename, content in uploaded.items():
    destination_path = os.path.join(pdf_folder, filename)
    with open(destination_path, "wb") as f:
        f.write(content)
    print(f"Saved to Drive: {destination_path}")

print("All PDFs saved permanently in Google Drive.")

"""OCR & Extract Text"""

from pdf2image import convert_from_path
import pytesseract
from PyPDF2 import PdfReader
from multiprocessing import Pool, cpu_count

def process_page(args):
    pdf_path, filename, page_num = args
    try:
        image = convert_from_path(pdf_path, dpi=150, first_page=page_num, last_page=page_num)[0]
        image_path = os.path.join(img_folder, f"{filename}_page_{page_num}.png")
        image.save(image_path, "PNG")
        text = pytesseract.image_to_string(image)
        return f"\n\n--- Page {page_num} of {filename} ---\n{text}"
    except Exception as e:
        return f"\n\n--- Page {page_num} of {filename} ---\nERROR: {e}"

def ocr_pdfs_in_drive(pdf_folder, output_file):
    tasks = []
    for filename in sorted(os.listdir(pdf_folder)):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(pdf_folder, filename)
            try:
                reader = PdfReader(pdf_path)
                total_pages = len(reader.pages)
                print(f"{filename} â†’ {total_pages} pages")
                for page_num in range(1, total_pages + 1):
                    tasks.append((pdf_path, filename, page_num))
            except Exception as e:
                print(f"Could not read {filename}: {e}")

    with Pool(cpu_count()) as pool:
        for result in pool.imap_unordered(process_page, tasks):
            with open(output_file, "a", encoding="utf-8") as f:
                f.write(result)

    print(f"OCR completed. Output saved to {output_file}")

ocr_text_file = os.path.join(drive_base, "ocr_output.txt")
ocr_pdfs_in_drive(pdf_folder, ocr_text_file)

with open(ocr_text_file, "r", encoding="utf-8") as f:
    ocr_text = f.read()

full_text_path = os.path.join(drive_base, "biology_full_content.txt")
with open(full_text_path, "w", encoding="utf-8") as f:
    f.write(ocr_text)

print(" Text saved in Google Drive.")

"""Split & Create Vector Store"""

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
docs = splitter.create_documents([ocr_text])

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embedding_model)

"""Save FAISS index"""

vectorstore.save_local(os.path.join(drive_base, "faiss_index"))
print(" FAISS index saved in Google Drive.")

"""Connect LLM + QA"""

from langchain_groq import ChatGroq
from langchain.chains import RetrievalQA

os.environ["GROQ_API_KEY"] = "your_groq_api_key_here"
llm = ChatGroq(model_name="llama3-70b-8192", temperature=0.2)
qa = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())

print(" Biology Chatbot Ready!")

"""Gradio Chat UI"""

import gradio as gr

def chat_with_bot(user_input, history):
    try:
        answer = qa.run(user_input)
        return answer
    except Exception as e:
        return f"Error: {str(e)}"

demo = gr.ChatInterface(
    fn=chat_with_bot,
    title="A/L Biology Chatbot",
    description="Ask any A/L Biology Questions.",
    theme="default",
)

demo.launch(share=True)

"""After Restart"""

!pip install gradio langchain langchain_groq langchain_community faiss-cpu sentence-transformers groq tiktoken

from google.colab import drive
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

drive.mount('/content/drive')
drive_base = "/content/drive/MyDrive/BioChatbot"

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.load_local(
    os.path.join(drive_base, "faiss_index"),
    embeddings=embedding_model,
    allow_dangerous_deserialization=True
)

print(" FAISS index loaded from Drive")

from langchain_groq import ChatGroq
from langchain.chains import RetrievalQA

os.environ["GROQ_API_KEY"] = "your_groq_api_key_here"
llm = ChatGroq(model_name="llama3-70b-8192", temperature=0.2)
qa = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())

print(" Biology Chatbot Ready (Reloaded)")

import gradio as gr

def chat_with_bot(user_input, history):
    try:
        answer = qa.run(user_input)
        return answer
    except Exception as e:
        return f"Error: {str(e)}"

demo = gr.ChatInterface(
    fn=chat_with_bot,
    title="A/L Biology Chatbot",
    description="Ask any A/L Biology Questions.",
    theme="default",
)

demo.launch(share=True)

"""# Test Process"""

test_questions = [
    {"q": "What is photosynthesis?",
     "a": "The process by which green plants use sunlight to synthesize food from carbon dioxide and water."},
    {"q": "Name the organelle responsible for ATP production.",
     "a": "Mitochondria"},
    {"q": "What are the stages of mitosis?",
     "a": "Prophase, metaphase, anaphase, and telophase"},
]

"""Semantic Accuracy"""

from sklearn.metrics.pairwise import cosine_similarity

def evaluate_similarity(bot_answer, expected_answer):
    emb_bot = embedding_model.embed_query(bot_answer)
    emb_expected = embedding_model.embed_query(expected_answer)
    return cosine_similarity([emb_bot], [emb_expected])[0][0]

scores = []
for item in test_questions:
    bot_answer = qa.run(item["q"])
    sim = evaluate_similarity(bot_answer, item["a"])
    scores.append(sim)
    print(f"\nQ: {item['q']}")
    print(f"Bot: {bot_answer}")
    print(f"Expected: {item['a']}")
    print(f"Similarity Score: {sim:.2f}")

print(f"\n Average Similarity Score: {sum(scores)/len(scores):.2f}")

"""LLM-Based Grading (LangChain Eval)"""

from langchain.evaluation.qa import QAEvalChain

examples = [{"query": t["q"], "answer": t["a"]} for t in test_questions]
predictions = [{"query": t["q"], "result": qa.run(t["q"])} for t in test_questions]

eval_chain = QAEvalChain.from_llm(llm)
graded = eval_chain.evaluate(examples, predictions)

for i, eg in enumerate(examples):
    print(f"\nQ: {eg['query']}")
    print(f"Correct: {eg['answer']}")
    print(f"Bot: {predictions[i]['result']}")
    print(f"Eval: {graded[i]['results']}")